----------------------- REVIEW 1 ---------------------
OVERALL EVALUATION: 2 (accept)
REVIEWER'S CONFIDENCE: 5 (expert)

----------- REVIEW -----------
In this paper, “OmniTune” is presented – a framework for tuning
program optimization parameters at runtime. OmniTune is based on a
machine learning approach and integrated for demonstration purposes in
SkelCL – an algorithmic skeleton library that aims to simplify
programming of multi-core devices.

The paper is well written and structured. The evaluation presents
interesting results for tuning the OpenCL work-group size. However,
since OpenCL applications have usually to be tuned in terms of various
optimization parameters (e.g., also for the chosen OpenCL vector data
type), it would have been desirable to see evaluation results also for
tuning more parameters than solely the work-group size.

Some points of minor criticism:

- It would be interesting to see the concrete performance advantage
  that is obtained when considering a three tier client-server model
  in contrast to a two tier model where the server is part of the
  remote entity.
- Probably, the definition of the “baseline work-group size” better
  fits in the evaluation section. Currently, in section 4.2, it
  remains unclear why this definition is needed.
- It would contribute to understanding if the types of the variables
  “x”, “p” and “y” in Section 3.3 are explicitly stated.


----------------------- REVIEW 2 ---------------------
OVERALL EVALUATION: 1 (weak accept)
REVIEWER'S CONFIDENCE: 4 (high)

----------- REVIEW -----------
The paper introduce Omnitune, an auto tuning framework managing run
time parameters in parallel GPU applications developed with
SkelCL. The framework can be paired to any system, apparently through
well defined interfaces. Sample results are shown that assess the
usability of the framework while auto tuning an important parameter of
GPU OpenCL applications (work group size).

The paper is well written. Although not being an expert of the
learning techniques used in the paper/framework, author arguments look
reasonable and the results reported at the end of the paper make sense
and look like a useful contribution. Personally, I would have liked to
see more experimental results achieved while running some kind of
application not used in the training/set up process of the
framework. After reading the paper I'm pretty sure it will work in the
general case, but I would definitively like to see more experiments to
validate the overall approach. Also, it would be nice to see what
happens when other kind of parameters are taken into account as the
target of the auto tuning process. Authors should also state whether
the framework may be used to attack the tuning of multiple parameters
at once.

Last but not least, the impact of the fact the tuned system is a
structured parallel programming framework should be better addressed
in the paper: what if other kind of code was considered (unstructured,
such as plain OpenCL code)?


----------------------- REVIEW 3 ---------------------
OVERALL EVALUATION: 1 (weak accept)
REVIEWER'S CONFIDENCE: 4 (high)

----------- REVIEW -----------
The paper presents an auto-tuning framework for run-time parameters,
based on a client-server model that decouples the parallel application
code, that only needs a light-weighted interface, from the
machine-learning logic in the server, whose results can be reused
across platforms, applications, and users.

The paper is neat and well written with only some minor flaws. It
motivates the case, and presents a nice idea and a technically sounded
solution. The experimental work is focused in a specific class of
application/skeleton, but as a reader I'm quite convinced that ideas
and conclusions can be extrapolated to many other cases.

Nevertheless, I have some concerns:

1) The discussion of results in sections 6.2 and 6.3 (specifically
those related to figures 10 and 11) assumes some generalizations that
are dangerous for the stated conclusions.

For example, in figure 10, workgroup size and shape properties are
represented summarizing the results in all platforms, for all
applications and datasets. The conclusion "There is no correlation
between workgroup size and performance" can be misleading. We know
that such correlations do exists due to architecture properties for
each application class in devices such as GPUs. Again in Figure 11,
results are summarized for example considering for each different
device, the performance obtained for all workgroup sizes
considered. This can hide correlations.  To be convincing, the
conclusions should be based on an analysis of the performance effect
of workgroup size parameters for each different device, or kernel, or
data-set.

2) Looking at the results, I'm not convinced that the workgroup
size/shape search space has been exhaustively defined and searched as
promised at the beginning of section 6.  It is not clear if the number
of rows, and columns has been artificially restricted (see e.g. Fig
12, where the maximum number of rows or columns is 80, really far from
potentially good configurations on for example Kepler Nvidia's GPUs,
where 128x4, 512x2, are valid and promising candidates).

In Section 4.2. Optimization parameters, you say that: "Typical values
are powers of two e.g. 1024, 4096, 8192..." This is not true for
example for Nvidia Fermi architecture (such as the GTX 590 board used
in your experiments), where the best performance can be found
sometimes for powers of two multiplied by 3.

3) When presenting the results in section 6.2, it is not clear what
the 429 scenarios represent, and the coverage of the
application/platform/data-set space.

The explanatory variables chosen are key for the machine-learning
process to produce valuable results. From the 102 explanatory
variables, it seems by the text in section 4.3 that most of them
describe the architecture. The code ones include only 3 generic ones,
and 2 related to the stencil skeleton. A couple of them more for the
datasets.  It is not discussed why these few ones are chosen, and how
can one expect them to relate to the code performance across really
different platforms (such as CPUs and GPUs).

4) It would be nice if the authors comment in the related work section
something about their previous work, specifically about MaSif (HiPC
2013 paper), as it should be complementary to this paper.

5) A minor concern that can be easily solved: In Motivation section 2,
the presentation omits several key details for the reader to
understand or being able to extrapolate the conclusions:
a) The exact architectures used are not specified (in Figure 2 the
reader cannot easily guess even if it is a CPU or a GPU kind of
device); b) the stencils used are not cited or commented.
