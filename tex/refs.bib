@INPROCEEDINGS{Enmyren2010,
  ABSTRACT = {We present SkePU, a C++ template library which provides a simple and unified interface for specifying data-parallel computations with the help of skeletons on GPUs using CUDA and OpenCL. The interface is also general enough to support other architectures, and SkePU implements both a sequential CPU and a parallel OpenMP backend. It also supports multi-GPU systems. Copying data between the host and the GPU device memory can be a performance bottleneck. A key technique in SkePU is the implementation of lazy memory copying in the container type used to represent skeleton operands, which allows to avoid unnecessary memory transfers. We evaluate SkePU with small benchmarks and a larger appli- cation, a Runge-Kutta ODE solver. The results show that a skeleton approach to GPU programming is viable, especially when the com- putation burden is large compared to memory I/O (the lazy memory copying can help to achieve this). It also shows that utilizing several GPUs have a potential for performance gains. We see that SkePU offers good performance with a more complex and realistic task such as ODE solving, with up to 10 times faster run times when using SkePU with a GPU backend compared to a sequential solver running on a fast CPU.},
  ANNOTATION = {SkePU is a data-parallel skeleton library with support for seq CPU, OpenMP, CUDA and OpenCL execution. It is a C++ template library which uses lazy copying to minimize the amount of data transferred between heterogeneous devices. Cited by 106.},
  AUTHOR = {Enmyren, J and Kessler, CW},
  BOOKTITLE = {HLPP},
  FILE = {:Users/cec/Google Drive/Mendeley/2010 - Enmyren, Kessler.pdf:pdf},
  KEYWORDS = {CUDA,Data Parallelism,GPU,OpenCL,Skeleton Programming},
  PAGES = {5--14},
  PUBLISHER = {ACM},
  TITLE = {{ SkePU: a multi-backend skeleton programming library for multi-GPU systems }},
  YEAR = {2010},
}

@INPROCEEDINGS{Marques2013,
  ABSTRACT = {The Graphics Processing Unit (GPU) is gaining popular- ity as a co-processor to the Central Processing Unit (CPU). However, harnessing its capabilities is a non-trivial exercise that requires good knowledge of parallel programming, more so when the complexity of these applications is increasingly rising. Languages such as StreamIt [1] and Lime [2] have addressed the offloading of composed computations to GPUs. However, to the best of our knowledge, no support exists at library level. To this extent, we propose Marrow, an algorithmic skeleton frame- work for the orchestration of OpenCL computations. Marrow expands the set of skeletons currently available for GPU computing, and enables their combination, through nesting, into complex structures. Moreover, it introduces optimizations that overlap communication and computa- tion, thus conjoining programming simplicity with performance gains in many application scenarios. We evaluated the framework from a perfor- mance perspective, comparing it against hand-tuned OpenCL programs. The results are favourable, indicating that Marrow's skeletons are both flexible and efficient in the context of GPU computing.},
  AUTHOR = {Marques, Ricardo and Paulino, Hervé and Alexandre, Fernando and Medeiros, Pedro D.},
  BOOKTITLE = {Euro-Par},
  DOI = {10.1007/978-3-642-40047-6_86},
  FILE = {:Users/cec/Google Drive/Mendeley/2013 - Marques et al.pdf:pdf},
  ISBN = {9783642400469},
  ISSN = {03029743},
  PAGES = {874--885},
  PUBLISHER = {Springer},
  TITLE = {{ Algorithmic skeleton framework for the orchestration of GPU computations }},
  VOLUME = {8097 LNCS},
  YEAR = {2013},
}

@INPROCEEDINGS{Kisuki,
  ANNOTATION = {Cited by 68.},
  AUTHOR = {Kisuki, T and Bohrweg, Niels and Beaulieu, Campus De},
  BOOKTITLE = {High Performance Computing},
  FILE = {:Users/cec/Google Drive/Mendeley/1999 - Kisuki, Bohrweg, Beaulieu.pdf:pdf},
  PAGES = {131--132},
  PUBLISHER = {Springer},
  TITLE = {{ A Feasibility Study in Iterative Compilation }},
  YEAR = {1999},
}

@INPROCEEDINGS{Steuwer2011,
  ABSTRACT = {While CUDA and OpenCL made general-purpose programming for Graphics Processing Units (GPU) popular, using these programming approaches remains complex and error-prone because they lack high-level abstractions. The especially challenging systems with multiple GPU are not addressed at all by these low-level programming models. We propose SkelCL – a library providing so-called algorithmic skeletons that capture recurring patterns of parallel compu- tation and communication, together with an abstract vector data type and constructs for specifying data distribution. We demonstrate that SkelCL greatly simplifies programming GPU systems. We report the competitive performance results of SkelCL using both a simple Mandelbrot set computation and an industrial-strength medical imaging application. Because the library is implemented using OpenCL, it is portable across GPU hardware of different vendors.},
  ANNOTATION = {SkelCL is a skeleton library for programming GPUs with OpenCL. The library supports programming of multiple GPUs and implements four data-parallel skeletons: Map, Zip, Reduce, and Scan. The skeleton abstractions create a { \~ { } } 5 { \% } overhead in performance, with a reduction in host program code size of { \~ { } } 4-20x. The design of the skeleton library seems solid. Particular nice features include the ability to supply muscle functions which accept arbitrary extra arguments, for example, a map function which accepts two arguments instead of one. Muscle functions are supplied as strings, which seems incredibly unsafe and prone to error, and significantly reduces the ability of editors/IDEs to detect errors.},
  AUTHOR = {Steuwer, Michel and Kegel, Philipp and Gorlatch, Sergei},
  BOOKTITLE = {IPDPSW},
  DOI = {10.1109/IPDPS.2011.269},
  FILE = {:Users/cec/Google Drive/Mendeley/2011 - Steuwer, Kegel, Gorlatch.pdf:pdf},
  ISBN = {978-1-61284-425-1},
  KEYWORDS = {Algorithmic Skeletons,CUDA,GPU Computing,GPU Programming,Multi-GPU Systems,OpenCL,SkelCL},
  MONTH = {may},
  PAGES = {1176--1182},
  PUBLISHER = {IEEE},
  TITLE = {{ SkelCL - A Portable Skeleton Library for High-Level GPU Programming }},
  YEAR = {2011},
}

@INPROCEEDINGS{Chen2014,
  ABSTRACT = {GPU is often equipped with complex memory sys- tems, including global memory, texture memory, shared memory, constant memory, and various levels of cache. Where to place the data is important for the performance of a GPU program. However, the decision is difficult for a programmer to make because of architecture complexity and the sensitivity of suitable data placements to input and architecture changes. This paper presents PORPLE, a portable data placement engine that enables a new way to solve the data placement problem. PORPLE consists of a mini specification language, a source-to-source compiler, and a runtime data placer. The language allows an easy description of a memory system; the compiler transforms a GPU program into a form amenable to runtime profiling and data placement; the placer, based on the memory description and data access patterns, identifies on the fly appropriate placement schemes for data and places them accordingly. PORPLE is distinctive in being adaptive to program inputs and architecture changes, being transparent to programmers (in most cases), and being extensible to new memory architectures. Our experiments on three types of GPU systems show that PORPLE is able to consistently find optimal or near-optimal placement despite the large differences among GPU architectures and program inputs, yielding up to 2.08X (1.59X on average) speedups on a set of regular and irregular GPU benchmarks.},
  ANNOTATION = {The authors have designed a specification language for describing GPU memory; a compiler for transforming GPU programs into a memory-agnostic form; a prediction model for assessing the efficiency of a given memory layout for a program; and a search engine for searching the space of possible memory layouts. Critical-reflection: The paper is generally solid. The requirement for a MSL spec for each GPU to be used limits portability, and how can the user validate that the spec is correct?},
  AUTHOR = {Chen, Guoyang and Wu, Bo},
  BOOKTITLE = {MICRO},
  FILE = {:Users/cec/Google Drive/Mendeley/2014 - Chen, Wu.pdf:pdf},
  KEYWORDS = {GPU,cache,compiler,data placement,hardware specification language},
  PAGES = {88--100},
  PUBLISHER = {IEEE},
  TITLE = {{ PORPLE: An Extensible Optimizer for Portable Data Placement on GPU }},
  YEAR = {2014},
}

@ARTICLE{Lutz2013,
  ABSTRACT = {GPGPUs are a powerful and energy-efficient solution for many problems.$\backslash$nFor higher performance or larger problems, it is necessary to distribute$\backslash$nthe problem across multiple GPUs, increasing the already high$\backslash$nprogramming complexity.$\backslash$nIn this article, we focus on abstracting the complexity of multi-GPU$\backslash$nprogramming for stencil computation. We show that the best strategy$\backslash$ndepends not only on the stencil operator, problem size, and GPU, but$\backslash$nalso on the PCI express layout. This adds nonuniform characteristics to$\backslash$na seemingly homogeneous setup, causing up to 23 { \% } performance loss. We$\backslash$naddress this issue with an autotuner that optimizes the distribution$\backslash$nacross multiple GPUs.},
  ANNOTATION = {An exhaustive search of the optimisation space for stencil benchmark border regions using features: PCI-type, halo size, compute granularity, { \# } of GPUs. Evaluated on 2 different mobos and GPUs. Optimisations evaluated: rotating volume so that largest dimension is first dimension, adjusting halo size dynamically, and computing halo region first so that swap can occur concurrently with the rest of the computation. They concluded that the optimal setting depends on problem size, stencil shape GPU, and PCI. This is well worth studying in detail as a case of successfully applied autotuning to GPUs. Cited by 19.},
  AUTHOR = {Lutz, Thibaut and Fensch, Christian and Cole, Murray},
  FILE = {:Users/cec/Google Drive/Mendeley/2013 - Lutz, Fensch, Cole.pdf:pdf},
  ISSN = {15443566},
  JOURNALTITLE = {TACO},
  NUMBER = {4},
  PAGES = {59},
  TITLE = {{ PARTANS: An Autotuning Framework for Stencil Computation on Multi-GPU Systems }},
  VOLUME = {9},
  YEAR = {2013},
}

@BOOK{Han2011,
  AUTHOR = {Han, Jiawei and Kamber, Micheline and Pei, Jian},
  PUBLISHER = {Elsevier},
  TITLE = {{ Data mining: concepts and techniques }},
  YEAR = {2011},
}

@INPROCEEDINGS{Georges2007,
  ABSTRACT = {Java performance is far from being trivial to benchmark because it is affected by various factors such as the Java application, its input, the virtual machine, the garbage collector, the heap size, etc. In addition, non-determinism at run-time causes the execution time of a Java program to differ from run to run. There are a number of sources of non-determinism such as Just-In-Time (JIT) compilation and optimization in the virtual machine (VM) driven by timer- based method sampling, thread scheduling, garbage collection, and various system effects. There exist a wide variety of Java performance evaluation methodologies used by researchers and benchmarkers. These methodologies differ from each other in a number of ways. Some report average performance over a number of runs of the same experiment; others report the best or second best performance ob- served; yet others report the worst. Some iterate the benchmark multiple times within a single VM invocation; others consider mul- tiple VM invocations and iterate a single benchmark execution; yet others consider multipleVMinvocations and iterate the benchmark multiple times. This paper shows that prevalent methodologies can be mis- leading, and can even lead to incorrect conclusions. The reason is that the data analysis is not statistically rigorous. In this pa- per, we present a survey of existing Java performance evaluation methodologies and discuss the importance of statistically rigorous data analysis for dealing with non-determinism. We advocate ap- proaches to quantify startup as well as steady-state performance, and, in addition, we provide the JavaStats software to automatically obtain performance numbers in a rigorous manner. Although this paper focuses on Java performance evaluation, many of the issues addressed in this paper also apply to other programming languages and systems that build on a managed runtime system.},
  ANNOTATION = {This paper describes a methodology for performing rigurous performance evaluation, focused on Java programs. This is a great paper to reference in your methodology section. Of particular interest is the discussion on calculating confidence intervals, using a normal distribution when n { > } = 30, else a t-distribution. This is the basis of srtime [1]. [1] https://github.com/ChrisCummins/srtime},
  AUTHOR = {Georges, Andy and Buytaert, Dries and Eeckhout, Lieven},
  BOOKTITLE = {OOPSLA},
  DOI = {10.1145/1297027.1297033},
  FILE = {:Users/cec/Google Drive/Mendeley/2007 - Georges, Buytaert, Eeckhout.pdf:pdf},
  ISBN = {9781595937865},
  ISSN = {03621340},
  KEYWORDS = {benchmarking,data analysis,java,methodolgy,statistics},
  LOCATION = {New York, NY, USA},
  MONTH = {oct},
  PUBLISHER = {ACM},
  TITLE = {{ Statistically Rigorous Java Performance Evaluation }},
  YEAR = {2007},
}

@INPROCEEDINGS{Bodin1998,
  ABSTRACT = {This paper investigates the applicability of iterative search techniques in program optimisation. Iterative com- pilation is usually considered too expensive for general pur- pose computing but is applicable to embedded applications where the cost is easily amortised over the number of em- bedded systems produced. This paper presents a case study, where an iterative search algorithm is used to investigate a non-linear transformation space and find the fastest execu- tion time within a fixed number of evaluations. By using profile feedback in the form of execution time, it searches a large but restricted transformation space and shows per- formance improvement over existing approaches. We show that in the case of large transformation spaces, we can achieve within 0.3 { \% } of the best possible time by visiting less then 0.25 { \% } of the space using a simple algorithm and find the minimum after visiting up to less than 1 { \% } of the space. 1.},
  ANNOTATION = {Cited by 117.},
  AUTHOR = {Bodin, F. and Kisuki, T. and Knijnenburg, P.M.W. and O'Boyle, Michael and Rohou, E.},
  BOOKTITLE = {PACT},
  FILE = {:Users/cec/Google Drive/Mendeley/1998 - Bodin et al.pdf:pdf},
  TITLE = {{ Iterative compilation in a non-linear optimisation space }},
  YEAR = {1998},
}

@INPROCEEDINGS{Agakov,
  ABSTRACT = {Iterative compiler optimization has been shown to out- perform static approaches. This, however, is at the cost of large numbers of evaluations of the program. This paper de- velops a new methodology to reduce this number and hence speed up iterative optimization. It uses predictive modelling from the domain of machine learning to automatically focus search on those areas likely to give greatest performance. This approach is independent of search algorithm, search space or compiler infrastructure and scales gracefully with the compiler optimization space size. Off-line, a training set of programs is iteratively evaluated and the shape of the spaces and program features are modelled. These models are learnt and used to focus the iterative optimization of a new program. We evaluate two learnt models, an indepen- dent and Markov model, and evaluate their worth on two embedded platforms, the Texas Instrument C6713 and the AMD Au1500. We show that such learnt models can speed up iterative search on large spaces by an order of magni- tude. This translates into an average speedup of 1.22 on the TI C6713 and 1.27 on the AMD Au1500 in just 2 evaluations.},
  ANNOTATION = {Introducing ML to iterative compilation. Cited by 316.},
  AUTHOR = {Agakov, F. and Bonilla, E. and Cavazos, J. and Franke, B. and Fursin, G. and O'Boyle, M. and Thomson, J. and Toussaint, M. and Williams, C.},
  BOOKTITLE = {CGO},
  DOI = {10.1109/CGO.2006.37},
  FILE = {:Users/cec/Google Drive/Mendeley/2006 - Agakov et al.pdf:pdf},
  ISBN = {0-7695-2499-0},
  PUBLISHER = {IEEE},
  TITLE = {{ Using Machine Learning to Focus Iterative Optimization }},
  YEAR = {2006},
}

@INPROCEEDINGS{Stephenson2003,
  ABSTRACT = {Compiler writers have crafted many heuristics over the years to approximately solve NP-hard problems efficiently. Find- ing a heuristic that performs well on a broad range of ap- plications is a tedious and difficult process. This paper in- troduces Meta Optimization, a methodology for automat- ically fine-tuning compiler heuristics. Meta Optimization uses machine-learning techniques to automatically search the space of compiler heuristics. Our techniques reduce com- piler design complexity by relieving compiler writers of the tedium of heuristic tuning. Our machine-learning system uses an evolutionary algorithm to automatically find effec- tive compiler heuristics. We present promising experimental results. In onemode of operationMeta Optimization creates application-specific heuristics which often result in impres- sive speedups. For hyperblock formation, one optimization we present in this paper, we obtain an average speedup of 23 { \% } (up to 73 { \% } ) for the applications in our suite. Further- more, by evolving a compiler's heuristic over several bench- marks, we can create effective, general-purpose heuristics. The best general-purpose heuristic our system found for hy- perblock formation improved performance by an average of 25 { \% } on our training set, and 9 { \% } on a completely unrelated test set. We demonstrate the efficacy of our techniques on three different optimizations in this paper: hyperblock for- mation, register allocation, and data prefetching.},
  ANNOTATION = {Cited by 221.},
  AUTHOR = {Stephenson, Mark and Martin, Martin and Reilly, Una-may O},
  BOOKTITLE = {PLDI},
  FILE = {:Users/cec/Google Drive/Mendeley/2003 - Stephenson, Martin, Reilly.pdf:pdf},
  ISBN = {1581136625},
  NUMBER = {5},
  PAGES = {77--90},
  TITLE = {{ Meta Optimization: Improving Compiler Heuristics with Machine Learning }},
  VOLUME = {38},
  YEAR = {2003},
}

@ARTICLE{Fursin2011,
  ABSTRACT = {Tuning compiler optimizations for rapidly evolving hardwaremakes port- ing and extending an optimizing compiler for each new platform extremely chal- lenging. Iterative optimization is a popular approach to adapting programs to a new architecture automatically using feedback-directed compilation. However, the large number of evaluations required for each program has prevented iterative compilation from widespread take-up in production compilers. Machine learning has been pro- posed to tune optimizations across programs systematically but is currently limited to a few transformations, long training phases and critically lacks publicly released, stable tools. Our approach is to develop a modular, extensible, self-tuning optimization infrastructure to automatically learn the best optimizations across multiple programs andarchitectures basedonthe correlationbetweenprogramfeatures, run-timebehavior and optimizations. In this paperwedescribeMilepostGCC, the first publicly-available open-source machine learning-based compiler. It consists of an Interactive Compila- tion Interface (ICI) and plugins to extract program features and exchange optimization data with the cTuning.org open public repository. It automatically adapts the inter- nal optimization heuristic at function-level granularity to improve execution time, code size and compilation time of a new program on a given architecture. Part of theMILEPOST technology together with low-level ICI-inspired plugin framework is now included in themainline GCC.We developed machine learning plugins based on probabilistic and transductive approaches to predict good combinations of optimiza- tions. Our preliminary experimental results show that it is possible to automatically reduce the execution time of individual MiBench programs, some by more than a fac- tor of 2,while also improving compilation time and code size. On average we are able to reduce the execution time of the MiBench benchmark suite by 11 { \% } for the ARC reconfigurable processor.We also present a realistic multi-objective optimization sce- nario for Berkeley DB library using Milepost GCC and improve execution time by approximately 17 { \% } , while reducing compilation time and code size by 12 { \% } and 7 { \% } respectively on Intel Xeon processor.},
  AUTHOR = {Fursin, Grigori and Kashnikov, Yuriy and Memon, Abdul Wahid and Chamski, Zbigniew and Temam, Olivier and Namolaru, Mircea and Yom-Tov, Elad and Mendelson, Bilha and Zaks, Ayal and Courtois, Eric and Bodin, Francois and Barnard, Phil and Ashton, Elton and Bonilla, Edwin and Thomson, John and Williams, Christopher K. I. and O'Boyle, Michael},
  DOI = {10.1007/s10766-010-0161-2},
  FILE = {:Users/cec/Google Drive/Mendeley/2011 - Fursin et al.pdf:pdf},
  ISSN = {0885-7458},
  JOURNALTITLE = {IJPP},
  KEYWORDS = {Adaptive compilation,Adaptive compiler,Automatic performance tuning,Collective optimization,Continuous optimization,Empirical performance tuning,Feedback-directed compilation,Iterative compilation,Machine learning,Machine learning compiler,Multi-objective optimization,Optimization prediction,Optimization repository,Portable optimization,Program characterization,Program features,Self-tuning compiler},
  MONTH = {jan},
  NUMBER = {3},
  PAGES = {296--327},
  PUBLISHER = {Springer},
  TITLE = {{ Milepost GCC: Machine Learning Enabled Self-tuning Compiler }},
  VOLUME = {39},
  YEAR = {2011},
}

@ARTICLE{Ryoo2008a,
  ABSTRACT = {GPUs have recently attracted the attention of many application developers as commodity data-parallel coprocessors. The newest generations of GPU architecture provide easier programmability and increased generality while maintaining the tremendous mem- ory bandwidth and computational power of traditional GPUs. This opportunity should redirect efforts inGPGPUresearch fromad hoc porting of applications to establishing principles and strategies that allow efficient mapping of computation to graphics hardware. In this work we discuss the GeForce 8800 GTX processor's organiza- tion, features, and generalized optimization strategies. Key to per- formance on this platform is using massive multithreading to uti- lize the large number of cores and hide global memory latency. To achieve this, developers face the challenge of striking the right balance between each thread's resource usage and the number of si- multaneously active threads. The resources to manage include the number of registers and the amount of on-chip memory used per thread, number of threads per multiprocessor, and global memory bandwidth. We also obtain increased performance by reordering accesses to off-chip memory to combine requests to the same or contiguous memory locations and apply classical optimizations to reduce the number of executed operations. We apply these strate- gies across a variety of applications and domains and achieve be- tween a 10.5X to 457X speedup in kernel codes and between 1.16X to 431X total application speedup.},
  ANNOTATION = {This is a really nice write-up of the challenges and approaches to optimising GPU programs. CUDA programming requires the developer to explicitly manage data layout in DRAM, caching, thread communication and other resources. Performance of such programs depends heavily on fully utilizing zero-overhead thread scheduling, memory bandwidth, thread grouping, shared control flow, and intra-block thread communication. The paper gives an example of optimising matrix multiplication by utilising shared memory through tiling, and loop unrolling. Cited by 772.},
  AUTHOR = {Ryoo, Shane and Rodrigues, Christopher I. and Baghsorkhi, Sara S. and Stone, Sam S. and Kirk, David B. and Hwu, Wen-mei W.},
  DOI = {10.1145/1345206.1345220},
  FILE = {:Users/cec/Google Drive/Mendeley/2008 - Ryoo et al(2).pdf:pdf},
  ISBN = {9781595937957},
  ISSN = {00778923},
  JOURNALTITLE = {PPoPP},
  KEYWORDS = {GPU computing,parallel computing},
  PAGES = {73},
  TITLE = {{ Optimization principles and application performance evaluation of a multithreaded GPU using CUDA }},
  YEAR = {2008},
}

@ARTICLE{Lee2010,
  ABSTRACT = {Recent advances in computing have led to an explosion in the amount of data being generated. Processing the ever-growing data in a timely manner has made throughput computing an important as- pect for emerging applications. Our analysis of a set of important throughput computing kernels shows that there is an ample amount of parallelism in these kernels which makes them suitable for to- day's multi-core CPUs and GPUs. In the past few years there have been many studies claiming GPUs deliver substantial speedups (be- tween 10X and 1000X) over multi-core CPUs on these kernels. To understand where such large performance difference comes from, we perform a rigorous performance analysis and find that after ap- plying optimizations appropriate for both CPUs and GPUs the per- formance gap between an Nvidia GTX280 processor and the Intel Core i7 960 processor narrows to only 2.5x on average. In this pa- per, we discuss optimization techniques for both CPU and GPU, analyze what architecture features contributed to performance dif- ferences between the two architectures, and recommend a set of architectural features which provide significant improvement in ar- chitectural efficiency for throughput kernels.},
  ANNOTATION = {Lee presents a performance analysis of optimised throughput computing applications for GPUs and CPUs. Of the 14 applications tested, they found GPU performance to be 0.7×-14.9× that of multi-threaded CPU code, with an average of only 2.5×. This is much lower than the 100×-1000× values reported by other studies, a fact that they attribute to uneven comparison of optimised GPU code to unoptimised CPU code, or vice versa. Lee et al. found that multithreading, cache blocking, reordering of memory accesses and use of SIMD instructions to contribute most to CPU performance. For GPUs, the most effective optimisations are reducing synchronization costs, and exploiting local shared memory. It is unclear whether this relative performance still holds after 5 years. Cited by 504.},
  AUTHOR = {Lee, Victor W. and Hammarlund, Per and Singhal, Ronak and Dubey, Pradeep and Kim, Changkyu and Chhugani, Jatin and Deisher, Michael and Kim, Daehyun and Nguyen, Anthony D. and Satish, Nadathur and Smelyanskiy, Mikhail and Chennupaty, Srinivas},
  DOI = {10.1145/1816038.1816021},
  FILE = {:Users/cec/Google Drive/Mendeley/2010 - Lee et al.pdf:pdf},
  ISBN = {9781450300537},
  ISSN = {01635964},
  JOURNALTITLE = {ACM SIGARCH Computer Architecture News},
  KEYWORDS = {cpu architecture,gpu architecture,mance measurement,perfor-,performance analysis,software optimization,throughput comput-},
  PAGES = {451},
  TITLE = {{ Debunking the 100X GPU vs. CPU myth }},
  VOLUME = {38},
  YEAR = {2010},
}

@INPROCEEDINGS{Magni2014,
  ABSTRACT = {OpenCL has been designed to achieve functional portability across multi-core devices from different vendors. However, the lack of a single cross-target optimizing compiler severely limits performance portability of OpenCL programs. Pro- grammers need to manually tune applications for each spe- cific device, preventing effective portability. We target a compiler transformation specific for data-parallel languages: thread-coarsening and show it can improve performance across different GPU devices. We then address the problem of se- lecting the best value for the coarsening factor parameter, i.e., deciding how many threads to merge together. We ex- perimentally show that this is a hard problem to solve: good configurations are difficult to find and naive coarsening in fact leads to substantial slowdowns. We propose a solution based on a machine-learning model that predicts the best coarsening factor using kernel-function static features. The model automatically specializes to the different architectures considered. We evaluate our approach on 17 benchmarks on four devices: two Nvidia GPUs and two different generations of AMD GPUs. Using our technique, we achieve speedups between 1.11× and 1.33× on average.},
  AUTHOR = {Magni, A. and Dubach, C. and O'Boyle, M.},
  BOOKTITLE = {PACT},
  DOI = {10.1145/2628071.2628087},
  FILE = {:Users/cec/Google Drive/Mendeley/2014 - Magni, Dubach, O'Boyle.pdf:pdf},
  ISBN = {9781450328098},
  ISSN = {1089795X},
  KEYWORDS = {opencl,optimization},
  PAGES = {455--466},
  TITLE = {{ Automatic Optimization of Thread-Coarsening for Graphics Processors }},
  YEAR = {2014},
}

@ARTICLE{Kamil2010,
  ABSTRACT = {Although stencil auto-tuning has shown tremendous potential in effectively utilizing architectural resources, it has hitherto been limited to single kernel instantiations; in addition, the large variety of stencil kernels used in practice makes this computation pattern difficult to assemble into a library. This work presents a stencil auto-tuning framework that significantly advances programmer productivity by automatically converting a straightforward sequential Fortran 95 stencil expression into tuned parallel implementations in Fortran, C, or CUDA, thus allowing performance portability across diverse computer architectures, including the AMD Barcelona, Intel Nehalem, Sun Victoria Falls, and the latest NVIDIA GPUs. Results show that our generalized methodology delivers significant performance gains of up to 22Ã speedup over the reference serial implementation. Overall we demonstrate that such domain-specific auto-tuners hold enormous promise for architectural efficiency, programmer productivity, performance portability, and algorithmic adaptability on existing and emerging multicore systems.},
  ANNOTATION = {Kamil presents an auto-tuning framework which accepts as input a Fortran 95 stencil expression, and generates tuned parallel implementations in Fortan, C, or CUDA. The system uses an IR to explore auto-tuning transformations, and has an SMP backend code generator. They demonstrate their system on 4 architectures using 3 benchmarks, with speedups of up to x22 over serial. The CUDA code generator *only uses global memory*. Also, there's no real search engine. They randomly enumerate a subset of the optimisation space, and then record only a *single execution time*, reporting the fastest. Cited by 127.},
  AUTHOR = {Kamil, Shoaib and Chan, Cy and Oliker, Leonid and Shall, John and Williams, Samuel},
  DOI = {10.1109/IPDPS.2010.5470421},
  FILE = {:Users/cec/Google Drive/Mendeley/2010 - Kamil et al.pdf:pdf},
  ISBN = {9781424464432},
  ISSN = {15302075},
  JOURNALTITLE = {IPDPS},
  TITLE = {{ An auto-tuning framework for parallel multicore stencil computations }},
  YEAR = {2010},
}

@ARTICLE{Collins2013,
  ABSTRACT = {Parallel skeletons provide a predefined set of parallel templates that can be combined, nested and parameterized with sequential code to produce complex parallel programs. The implementation of each skeleton includes parameters that have a significant effect on performance; so carefully tuning them is vital. The optimization space formed by these parameters is complex, non-linear, exhibits multiple local optima and is program dependent. This makes manual tuning impractical. Ef- fective automatic tuning is therefore essential for the performance of parallel skeleton programs. In this paper we present MaSiF, a novel tool to auto-tune the parallelization parameters of skeleton parallel programs. It reduces the size of the parameter space using a combination of machine learning, via nearest neighbor classification, and linear dimensionality reduction using Principal Components Analysis. To auto-tune a new program, a set of program features is determined statically and used to compute k nearest neighbors from a set of training programs. Previously collected performance data for the nearest neighbors is used to reduce the size of the search space using Principal Components Analysis. Good parallelization parameters are found quickly by searching this smaller search space.We evaluate MaSiF for two existing parallel frameworks: Threading Building Blocks and FastFlow. MaSiF achieves 89 { \% } of the performance of the oracle on average. This exploration requires just 45 parameters values on average, which is { \~ { } } 0.05 { \% } of the optimization space. In contrast, a state-of-the- art machine learning approach achieves 51 { \% } . MaSiF achieves an average speedup of 1.32× over parallelization parameters chosen by human experts.},
  ANNOTATION = {MaSiF is a tool that selects optimal paramaters for a skeleton program, by using machine learning to compare its feature vector against k-nearest neighbours of a training dataset. PCA is used to reduce the parameter optimisation space for Thread Building Blocks and FastFlow, and the performance is compared against a human expert, oracle, and best competing method. The paper is well written and contains a susbstial experimental results section. The assumption of statically unknowable properties is a bit of an oversight. Could be improved using dynamic autotuning?},
  AUTHOR = {Collins, Alexander and Fensch, Christian and Leather, Hugh and Cole, Murray},
  DOI = {10.1109/HiPC.2013.6799098},
  FILE = {:Users/cec/Google Drive/Mendeley/2013 - Collins et al(2).pdf:pdf},
  ISBN = {978-1-4799-0730-4},
  JOURNALTITLE = {HiPC},
  MONTH = {dec},
  PAGES = {186--195},
  PUBLISHER = {IEEE},
  TITLE = {{ MaSiF: Machine Learning Guided Auto-tuning of Parallel Skeletons }},
  YEAR = {2013},
}

@INPROCEEDINGS{Ansel2013,
  AUTHOR = {Ansel, J and Kamil, S and Veeramachaneni, K and Reilly, U. O and Amarasinghe, S. A},
  BOOKTITLE = {PACT},
  FILE = {:Users/cec/Google Drive/Mendeley/2013 - Ansel et al.pdf:pdf},
  TITLE = {{ OpenTuner: An Extensible Framework for Program Autotuning }},
  YEAR = {2013},
}

@ARTICLE{Fursin2014,
  ABSTRACT = {Empirical auto-tuning and machine learning techniques have been showing high potential to improve execution time, power consumption, code size, reliability and other important metrics of various applications for more than two decades. However, they are still far from widespread production use due to lack of native support for auto-tuning in an ever changing and complex software and hardware stack, large and multi-dimensional optimization spaces, excessively long exploration times, and lack of unified mechanisms for preserving and sharing of optimization knowledge and research material. We present a possible collaborative approach to solve above problems using Collective Mind knowledge management system. In contrast with previous cTuning framework, this modular infrastructure allows to preserve and share through the Internet the whole auto-tuning setups with all related artifacts and their software and hardware dependencies besides just performance data. It also allows to gradually structure, systematize and describe all available research material including tools, benchmarks, data sets, search strategies and machine learning models. Researchers can take advantage of shared components and data with extensible meta-description to quickly and collaboratively validate and improve existing auto-tuning and benchmarking techniques or prototype new ones. The community can now gradually learn and improve complex behavior of all existing computer systems while exposing behavior anomalies or model mispredictions to an interdisciplinary community in a reproducible way for further analysis. We present several practical, collaborative and model-driven auto-tuning scenarios. We also decided to release all material at c-mind.org/repo to set up an example for a collaborative and reproducible research as well as our new publication model in computer engineering where experimental results are continuously shared and validated by the community.},
  ANNOTATION = {This paper presents a collaborative and big-data driven approach to auto-tuning, which allows for researchers to share and evaluate ML and DM results results. The paper argues that the inability to develop auto-tuning and machine learning tools can be attributable to: * Lack of common, diverse benchmarks and data sets. * Lack of common experimental methodology. * Problems with continuously changing hardware/software stack. * Difficulty to reproduce results due to lack of full HW/SW dependencies. * Difficulty to validate techniques due to lack of sharing in publications. Cited by 5.},
  AUTHOR = {Fursin, Grigori and Miceli, Renato and Lokhmotov, Anton and Gerndt, Michael and Baboulin, Marc and Malony, Allen D and Chamski, Zbigniew and Novillo, Diego and { Del Vento } , Davide},
  FILE = {:Users/cec/Google Drive/Mendeley/2014 - Fursin et al.pdf:pdf},
  JOURNALTITLE = {Scientific Programming},
  KEYWORDS = {NoSQL repository,agile development,big data driven optimization,code and data sharing,collaborative experimentation,collaborative knowledge management,data mining,high performance computing,machine learning,model driven optimization,modeling of computer behavior,multi-objective optimization,open access publication model,performance prediction,performance regression buildbot,plugin-based tuning,public repository of knowledge,reproducible research,specification sharing,systematic auto-tuning,systematic benchmarking},
  NUMBER = {4},
  PAGES = {309--329},
  PUBLISHER = {IOS Press},
  TITLE = {{ Collective Mind: Towards practical and collaborative auto-tuning }},
  VOLUME = {22},
  YEAR = {2014},
}

